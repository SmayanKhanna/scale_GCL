{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a602c7d-5b97-4985-a73a-7e4d72526abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from itertools import combinations\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Subset, Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader as RegularDataLoader\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data, HeteroData, Batch\n",
    "from torch_geometric.loader import DataLoader \n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "\n",
    "\n",
    "# GNN Layers\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATConv, SAGEConv, GINConv, HeteroConv,\n",
    "    global_mean_pool, global_max_pool, global_add_pool,\n",
    "    DMoNPooling\n",
    ")\n",
    "\n",
    "# Graph Utilities\n",
    "from torch_geometric.utils import (\n",
    "    k_hop_subgraph, subgraph, from_networkx,\n",
    "    to_networkx, to_dense_adj, to_dense_batch,\n",
    "    degree\n",
    ")\n",
    "\n",
    "#sk-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Dense Graph Utilities\n",
    "from torch_geometric.nn.dense.mincut_pool import _rank3_trace\n",
    "\n",
    "# === Visualization & Analysis ===\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import torchviz\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264c7feb-6a3b-4eae-bfa9-8ded287ab3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fa02c-607a-4954-ae04-9bf9b5ed877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset.\n"
     ]
    }
   ],
   "source": [
    "# dataset_name = 'ENZYMES'\n",
    "# dataset_name = 'MUTAG'\n",
    "# dataset_name = 'REDDIT-BINARY'\n",
    "# dataset_name = 'IMDB-BINARY'\n",
    "# dataset_name = 'MUTAG'\n",
    "# dataset_name = 'PROTEINS'\n",
    "# dataset_name = 'DD'\n",
    "dataset_name = 'NCI1'\n",
    "\n",
    "#downloads the dataset/loads it\n",
    "dataset_path = f\"data/TUDataset/{dataset_name}\"\n",
    "if not os.path.exists(os.path.join(dataset_path, \"processed\", \"data.pt\")):\n",
    "    print(\"Dataset not found — downloading + processing.\")\n",
    "    dataset = TUDataset(root=\"data/TUDataset\", name=dataset_name)\n",
    "else:\n",
    "    print(\"Loading cached dataset.\")\n",
    "    dataset = TUDataset(root=\"data/TUDataset\", name=dataset_name)\n",
    "\n",
    "num_features = max(dataset.num_features, 1)\n",
    "num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30ade24-5625-4d40-b2ad-aa7b1bd68e03",
   "metadata": {},
   "source": [
    "# Experiment 1: Unsupervised Embedding Evaluation\n",
    "\n",
    "This a very simple experiment - we simply run an untrained GIN, extract embeddings and use the linear evaluation protocols that many \"GCL-style\" papers use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2158767e-929f-41b4-95c9-ed49d1b555a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the architecture that https://github.com/sunfanyunn/InfoGraph/blob/master/unsupervised/gin.py uses (and 99% of graph-classification-based GCL methods)\n",
    "class GINBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    GIN block: LazyLinear→ReLU→Linear inside GINConv, then BN+ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            # nn.LazyLinear(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.gin = GINConv(self.mlp)\n",
    "        self.bn  = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gin(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        # return F.relu(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_features, dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = num_features if i==0 else dim\n",
    "            self.blocks.append(GINBlock(in_dim, dim))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        if x is None:\n",
    "            x = torch.ones(batch.shape[0]).to(device)\n",
    "            \n",
    "        xs = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x, edge_index)       # GIN → BN → ReLU\n",
    "            xs.append(x)\n",
    "\n",
    "        pooled = [global_add_pool(h, batch) for h in xs]\n",
    "        return torch.cat(pooled, dim=1), x\n",
    "\n",
    "    def get_embeddings(self, loader):\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        ret = []\n",
    "        y = []\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data.to(device)\n",
    "                x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "                if x is None:\n",
    "                    x = torch.ones((batch.shape[0],1)).to(device)\n",
    "                x, _ = self.forward(x, edge_index, batch)\n",
    "                ret.append(x.cpu().numpy())\n",
    "                y.append(data.y.cpu().numpy())\n",
    "        ret = np.concatenate(ret, 0)\n",
    "        y = np.concatenate(y, 0)\n",
    "        return ret, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf6c3d-a8a7-4d25-96c9-59deb9d6ec86",
   "metadata": {},
   "source": [
    "### A barebones way of doing this: (initializes a new random GNN each iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5af38481-0470-4a3a-83e1-3465bb4ecda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "num_pooling_layers = 3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19670d20-1ee8-43d0-8237-c26ec7c8c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "acc_vals = []\n",
    "seeds = []\n",
    "for _ in range(num_trials):\n",
    "\n",
    "    seed = random.randint(100, 999)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    seeds.append(seed) \n",
    "    \n",
    "    model = Encoder(num_features, hidden_dim, num_pooling_layers)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    emb, y = model.get_embeddings(loader)\n",
    "    acc, acc_val = evaluate_embedding(emb, y, device=device, search=True)\n",
    "    acc_vals.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b74aeb5-ec0f-40a8-ba2e-e7e883f3478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MUTAG\n",
      "Classification Accuracy: 88.85 ±  1.31\n",
      "[276, 481, 794, 415, 893]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Classification Accuracy: {np.mean(acc_vals) * 100:.2f} ±  {np.std(acc_vals) * 100:.2f}\")\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c2ab5-ddb4-4788-8891-b21e05636918",
   "metadata": {},
   "source": [
    "## Experiment 1.1: MLP benchmark: we borrow code from Fair Evaluation from Graph classification: https://github.com/diningphil/gnn-comparison/blob/master/models/graph_classifiers/MolecularFingerprint.py\n",
    "\n",
    "#### ONLY WORKS FOR MOLECULAR DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ae3ec13c-8486-43ed-a318-60c35cabf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularFingerprint(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_features, hidden_dim):\n",
    "        super(MolecularFingerprint, self).__init__()\n",
    "        \n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(dim_features, hidden_dim), nn.ReLU(),\n",
    "                                       torch.nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, batch = data.x, data.batch\n",
    "        if x is None:\n",
    "            x = torch.ones(batch.shape[0])\n",
    "            \n",
    "        return self.mlp(global_add_pool(x, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a3642-b7ae-4d39-99dd-3697e855e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_simple(model, loader):\n",
    "    ret = []\n",
    "    y = []\n",
    "    \n",
    "    for data in loader:\n",
    "        y.append(data.y.detach().numpy())\n",
    "        ret.append(model(data).detach().numpy())\n",
    "\n",
    "    emb = np.concatenate(ret, 0)\n",
    "    y = np.concatenate(y, 0)\n",
    "    \n",
    "    return emb, y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6348e018-b3dc-46bd-91eb-4ebd3f098c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_trials = 5\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "acc_vals = []\n",
    "seeds = []\n",
    "for _ in range(num_trials):\n",
    "\n",
    "    seed = random.randint(100, 999)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    seeds.append(seed) \n",
    "    \n",
    "    model = MolecularFingerprint(num_features, hidden_dim=256)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    emb, y = extract_embeddings_simple(model, loader)\n",
    "    acc, acc_val = evaluate_embedding(emb, y, device=device, search=True)\n",
    "    acc_vals.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "04202f13-c39c-47ca-a4ac-16acf1bf6588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: NCI1\n",
      "Classification Accuracy: 69.43 ±  0.10\n",
      "[467, 358, 883, 669, 409]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Classification Accuracy: {np.mean(acc_vals) * 100:.2f} ±  {np.std(acc_vals) * 100:.2f}\")\n",
    "print(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54fd875-3f1b-4245-bded-69837fffc471",
   "metadata": {},
   "source": [
    "# Experiment 2: Trivial Statistics\n",
    "\n",
    "A very simple experiment - a powerful GNN can easily extract powerful features that correlate strongly with the feature statistics of graphs!\n",
    "\n",
    "These features, when trained on, can achieve comparable graph classification accuracy to SOTA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5721949-3f4c-4334-aebd-d9cb71aa11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "for data in dataset:\n",
    "    n = data.num_nodes\n",
    "    e = data.edge_index.size(1)\n",
    "    avg_deg = 2 * e / n\n",
    "    degs = degree(data.edge_index[0], num_nodes=n).cpu().numpy()\n",
    "    # G = to_networkx(data)\n",
    "    # n_comp = nx.average_clustering(G)\n",
    "    hist, _ = np.histogram(degs, bins=[0,1,2,3,4,5, np.inf])\n",
    "\n",
    "    stats.append([n, avg_deg] + hist.tolist())\n",
    "    # stats.append([n, avg_deg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e45e0e8-b841-4caa-8a0e-8fed98b5ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats1 = np.array(stats)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04539111-146d-4df8-96ce-d884ef6a4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.array(stats)\n",
    "labels_list = [data.y.item() for data in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae01bc5-5530-422d-9bde-6abd7bce0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_trivial_baseline(stats_s, labs_s, seed):\n",
    "    # the eval funciton\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "    param_grid = {'svc__C': [0.001,0.01,0.1,1,10,100,1000]}\n",
    "    accs = []\n",
    "    for tr, te in kf.split(stats_s, labs_s):\n",
    "        gs = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
    "        gs.fit(stats_s[tr], [labs_s[i] for i in tr])\n",
    "        accs.append(accuracy_score([labs_s[i] for i in te], gs.predict(stats_s[te])))\n",
    "    return np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00777cbe-6b9f-4a1d-8f7b-f029f79ec95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_count      → 62.65% ± 0.18%\n",
      "avg_degree      → 55.91% ± 0.20%\n",
      "deg_hist        → 68.24% ± 0.36%\n",
      "all_trivial     → 68.50% ± 0.47%\n",
      "random          → 49.18% ± 1.06%\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "for data in dataset:\n",
    "    G = to_networkx(data)  \n",
    "    n = data.num_nodes\n",
    "    e = data.edge_index.size(1)\n",
    "    avg_deg = 2*e/n\n",
    "    degs = degree(data.edge_index[0], num_nodes=n).cpu().numpy()\n",
    "    hist, _ = np.histogram(degs, bins=[0,1,2,3,4,5,np.inf])\n",
    "\n",
    "    # # extra stats:\n",
    "    # clust = nx.average_clustering(G)\n",
    "    # assort = nx.degree_assortativity_coefficient(G)\n",
    "    stats.append([n, avg_deg] + hist.tolist())\n",
    "\n",
    "stats = np.array(stats)\n",
    "labels = np.array([d.y.item() for d in dataset])\n",
    "\n",
    "feature_sets = {\n",
    "  \"node_count\":        [0],\n",
    "  \"avg_degree\":        [1],\n",
    "  \"deg_hist\":          list(range(2, 2+len(hist))),\n",
    "  # \"clust+comps+assort\":[2+len(hist), 2+len(hist)+1],\n",
    "  \"all_trivial\":       list(range(stats.shape[1])),\n",
    "  \"random\":            None  # handled specially\n",
    "}\n",
    "\n",
    "def eval_set(name, X, y, seed):\n",
    "    if name==\"random\":\n",
    "        X = np.random.RandomState(seed).randn(*X.shape)\n",
    "    else:\n",
    "        cols = feature_sets[name]\n",
    "        X = X[:, cols]\n",
    "    return eval_trivial_baseline(X, y, seed)\n",
    "\n",
    "#ablations\n",
    "results = {}\n",
    "for name in feature_sets:\n",
    "    accs = []\n",
    "    for seed in seeds:\n",
    "        random.seed(seed); np.random.seed(seed)\n",
    "        idx = np.random.permutation(len(stats))\n",
    "        accs.append(eval_set(name, stats[idx], labels[idx], seed))\n",
    "    results[name] = (np.mean(accs), np.std(accs))\n",
    "\n",
    "#tabulations\n",
    "for name, (mean, std) in results.items():\n",
    "    print(f\"{name:15s} → {mean*100:.2f}% ± {std*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fec19961-e97b-49b8-9b90-c10eae0bb9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[237, 206, 982, 445, 207]\n"
     ]
    }
   ],
   "source": [
    "print(seeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitenv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
